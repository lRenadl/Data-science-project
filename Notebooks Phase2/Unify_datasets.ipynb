{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22mOtVPviQGS",
        "outputId": "8d567f5a-c473-44b9-c7de-912c30772e9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved:\n",
            "  - reddit_standardized.csv  ((135, 10))\n",
            "  - twitter_standardized.csv ((90, 10))\n",
            "  - youtube_standardized.csv      ((100, 10))\n",
            "  - all_platforms_standardized.csv     ((325, 10))\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Unify Reddit, Twitter, and YouTube datasets to a common schema (Twitter/YouTube style)\n",
        "and save both per-platform standardized files and one combined file.\n",
        "\n",
        "Target schema/column order:\n",
        "['Platform','URL','Engagement','Text','Matched Keywords',\n",
        " 'toxicity_score','Label','clean_text','word_count','negative_word_count']\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------- INPUT PATHS (adjust if needed) ----------\n",
        "P_REDDIT  = \"reddit_labeled.csv\"\n",
        "P_TWITTER = \"twitter_labeled (1).csv\"\n",
        "P_YT      = \"youtube_cleaned_balanced.csv\"\n",
        "\n",
        "# ---------- OUTPUT PATHS ----------\n",
        "OUT_REDDIT  = \"reddit_standardized.csv\"\n",
        "OUT_TWITTER = \"twitter_standardized.csv\"\n",
        "OUT_YT      = \"youtube_standardized.csv\"\n",
        "OUT_ALL     = \"all_platforms_standardized.csv\"\n",
        "\n",
        "TARGET_COLS = [\n",
        "    \"Platform\",\"URL\",\"Engagement\",\"Text\",\"Matched Keywords\",\n",
        "    \"toxicity_score\",\"Label\",\"clean_text\",\"word_count\",\"negative_word_count\"\n",
        "]\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def ensure_list(x):\n",
        "    \"\"\"Ensure Matched Keywords is a list-like (handles '[]', comma strings, single token).\"\"\"\n",
        "    if isinstance(x, list): return x\n",
        "    if pd.isna(x): return []\n",
        "    s = str(x).strip()\n",
        "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
        "        inner = s[1:-1].strip()\n",
        "        if not inner: return []\n",
        "        parts = [p.strip().strip(\"'\\\"\") for p in inner.split(\",\")]\n",
        "        return [p for p in parts if p]\n",
        "    if \",\" in s:\n",
        "        return [t.strip() for t in s.split(\",\") if t.strip()]\n",
        "    return [] if s in {\"\", \"none\", \"nan\"} else [s]\n",
        "\n",
        "def finalize(df):\n",
        "    \"\"\"Coerce dtypes, compute missing word_count, fill missing negative_word_count, and de-dupe.\"\"\"\n",
        "    df[\"Platform\"] = df[\"Platform\"].astype(str)\n",
        "    df[\"URL\"] = df[\"URL\"].astype(str)\n",
        "\n",
        "    # Engagement numeric\n",
        "    df[\"Engagement\"] = pd.to_numeric(df[\"Engagement\"], errors=\"coerce\").fillna(0)\n",
        "\n",
        "    # Matched Keywords as list (store as stringified list for CSV or keep lists if you prefer)\n",
        "    df[\"Matched Keywords\"] = df[\"Matched Keywords\"].apply(ensure_list)\n",
        "\n",
        "    # toxicity_score float\n",
        "    df[\"toxicity_score\"] = pd.to_numeric(df[\"toxicity_score\"], errors=\"coerce\")\n",
        "\n",
        "    # Label: keep as-is if present; if missing and toxicity available, derive by threshold=0.5\n",
        "    if \"Label\" not in df.columns:\n",
        "        df[\"Label\"] = (df[\"toxicity_score\"] >= 0.5).astype(int)\n",
        "    else:\n",
        "        # If Label exists but has NaN, fill from threshold\n",
        "        miss = df[\"Label\"].isna()\n",
        "        if miss.any():\n",
        "            df.loc[miss, \"Label\"] = (df.loc[miss, \"toxicity_score\"] >= 0.5).astype(int)\n",
        "\n",
        "    # word_count: compute from clean_text if missing/NaN\n",
        "    if \"word_count\" not in df.columns:\n",
        "        df[\"word_count\"] = df[\"clean_text\"].astype(str).str.split().apply(len)\n",
        "    else:\n",
        "        wc = pd.to_numeric(df[\"word_count\"], errors=\"coerce\")\n",
        "        missing_wc = wc.isna()\n",
        "        wc = wc.fillna(0)\n",
        "        if missing_wc.any():\n",
        "            wc.loc[missing_wc] = df.loc[missing_wc, \"clean_text\"].astype(str).str.split().apply(len)\n",
        "        df[\"word_count\"] = wc.astype(int)\n",
        "\n",
        "    # negative_word_count: if not present, set 0 for now\n",
        "    if \"negative_word_count\" not in df.columns:\n",
        "        df[\"negative_word_count\"] = 0\n",
        "    else:\n",
        "        df[\"negative_word_count\"] = pd.to_numeric(df[\"negative_word_count\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    # Minimal de-duplication on (URL, clean_text)\n",
        "    df = df.drop_duplicates(subset=[\"URL\", \"clean_text\"], keep=\"first\")\n",
        "\n",
        "    # Reorder columns\n",
        "    return df[TARGET_COLS]\n",
        "\n",
        "# ---------- standardizers ----------\n",
        "def standardize_reddit(df):\n",
        "    out = pd.DataFrame()\n",
        "    out[\"Platform\"] = df.get(\"Platform\", \"Reddit\")\n",
        "    out[\"URL\"] = df.get(\"URL\", \"\")\n",
        "    out[\"Engagement\"] = df.get(\"Engagement\", 0)\n",
        "\n",
        "    # Reddit file may not have raw 'Text'; fallback to 'Clean Text'\n",
        "    out[\"Text\"] = df[\"Text\"] if \"Text\" in df.columns else df.get(\"Clean Text\", \"\")\n",
        "\n",
        "    out[\"Matched Keywords\"] = df.get(\"Matched Keywords\", \"[]\")\n",
        "    out[\"toxicity_score\"] = df.get(\"Toxicity Score\", np.nan)  # map to target name\n",
        "    out[\"Label\"] = df.get(\"Label\", np.nan)\n",
        "    out[\"clean_text\"] = df.get(\"Clean Text\", \"\")\n",
        "    out[\"word_count\"] = df.get(\"Word Count\", np.nan)\n",
        "    # Reddit typically lacks negative_word_count; create it\n",
        "    out[\"negative_word_count\"] = 0\n",
        "    return finalize(out)\n",
        "\n",
        "def standardize_twitter(df):\n",
        "    out = pd.DataFrame()\n",
        "    out[\"Platform\"] = df.get(\"Platform\", \"Twitter\")\n",
        "    out[\"URL\"] = df.get(\"URL\", \"\")\n",
        "    out[\"Engagement\"] = df.get(\"Engagement\", 0)\n",
        "    out[\"Text\"] = df.get(\"Text\", \"\")\n",
        "    out[\"Matched Keywords\"] = df.get(\"Matched Keywords\", \"[]\")\n",
        "    out[\"toxicity_score\"] = df.get(\"toxicity_score\", np.nan)\n",
        "    out[\"Label\"] = df.get(\"Label\", np.nan)\n",
        "    out[\"clean_text\"] = df.get(\"clean_text\", \"\")\n",
        "    out[\"word_count\"] = df.get(\"word_count\", np.nan)\n",
        "    out[\"negative_word_count\"] = df.get(\"negative_word_count\", 0)\n",
        "    return finalize(out)\n",
        "\n",
        "def standardize_youtube(df):\n",
        "    out = pd.DataFrame()\n",
        "    out[\"Platform\"] = df.get(\"Platform\", \"YouTube\")\n",
        "    # Reddit/Twitter use \"URL\"; YouTube has \"Video URL\"\n",
        "    out[\"URL\"] = df.get(\"URL\", df.get(\"Video URL\", \"\"))\n",
        "    out[\"Engagement\"] = df.get(\"Engagement\", 0)\n",
        "    out[\"Text\"] = df.get(\"Comment\", \"\")\n",
        "    out[\"Matched Keywords\"] = df.get(\"Matched Keywords\", \"[]\")\n",
        "    out[\"toxicity_score\"] = df.get(\"toxicity_score\", np.nan)\n",
        "    out[\"Label\"] = df.get(\"Label\", np.nan)\n",
        "    out[\"clean_text\"] = df.get(\"clean_text\", \"\")\n",
        "    out[\"word_count\"] = df.get(\"word_count\", np.nan)\n",
        "    out[\"negative_word_count\"] = df.get(\"negative_word_count\", 0)\n",
        "    return finalize(out)\n",
        "\n",
        "# ---------- load, standardize, save ----------\n",
        "reddit  = pd.read_csv(P_REDDIT)\n",
        "twitter = pd.read_csv(P_TWITTER)\n",
        "yt      = pd.read_csv(P_YT)\n",
        "\n",
        "reddit_std  = standardize_reddit(reddit)\n",
        "twitter_std = standardize_twitter(twitter)\n",
        "yt_std      = standardize_youtube(yt)\n",
        "\n",
        "reddit_std.to_csv(OUT_REDDIT,  index=False)\n",
        "twitter_std.to_csv(OUT_TWITTER, index=False)\n",
        "yt_std.to_csv(OUT_YT,          index=False)\n",
        "\n",
        "all_std = pd.concat([reddit_std, twitter_std, yt_std], ignore_index=True)\n",
        "all_std.to_csv(OUT_ALL, index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(f\"  - {OUT_REDDIT}  ({reddit_std.shape})\")\n",
        "print(f\"  - {OUT_TWITTER} ({twitter_std.shape})\")\n",
        "print(f\"  - {OUT_YT}      ({yt_std.shape})\")\n",
        "print(f\"  - {OUT_ALL}     ({all_std.shape})\")\n"
      ]
    }
  ]
}