{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p32yjRhD8pQJ",
        "outputId": "249f58ea-0388-4285-fafd-4d79a221ff52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.12/dist-packages (4.16.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (3.3.1)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tweepy pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† ØªÙˆÙŠØªØ±"
      ],
      "metadata": {
        "id": "5qlXBN4gBB5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "# -------- 1) Twitter API Authentication --------\n",
        "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAMpF4QEAAAAA4C3hA1G%2FhaVaD5z8Ygd3UCkC1Z8%3D2yAo5a1LQV3pCtEhbptnDzQQ5qrAfaqz5LzLHUhRyh9xH60YDj\"\n",
        "client = tweepy.Client(bearer_token=bearer_token)\n",
        "\n",
        "# -------- 2) Keywords --------\n",
        "main_keywords = [\n",
        "    \"hate speech\",\n",
        "    \"go back to your country\",\n",
        "    \"immigrants\",\n",
        "    \"refugees\",\n",
        "    \"politicians are corrupt\",\n",
        "    \"racism\",\n",
        "    \"racial slurs\",\n",
        "    \"white supremacy\",\n",
        "    \"black people\",\n",
        "    \"Asian people\",\n",
        "    \"islamophobia\",\n",
        "    \"anti-semitism\",\n",
        "    \"religious hate\",\n",
        "    \"Muslims\",\n",
        "    \"Jews\",\n",
        "    \"Christians\",\n",
        "    \"misogyny\"\n",
        "]\n",
        "\n",
        "sentiment_keywords = [\n",
        "    \"disgusting\",\n",
        "    \"horrible\",\n",
        "    \"unacceptable\",\n",
        "    \"shameful\",\n",
        "    \"evil\"\n",
        "]\n",
        "\n",
        "all_keywords = main_keywords + sentiment_keywords\n",
        "\n",
        "# -------- 3) Search Tweets --------\n",
        "query = \" OR \".join(all_keywords) + \" -is:retweet lang:en\"\n",
        "\n",
        "tweets = client.search_recent_tweets(\n",
        "    query=query,\n",
        "    tweet_fields=[\"public_metrics\", \"created_at\"],\n",
        "    max_results=100\n",
        ")\n",
        "\n",
        "# -------- 4) Prepare Data --------\n",
        "data = []\n",
        "if tweets.data:\n",
        "    for tweet in tweets.data:\n",
        "        tweet_url = f\"https://twitter.com/user/status/{tweet.id}\"\n",
        "\n",
        "        metrics = tweet.public_metrics\n",
        "        engagement = metrics.get(\"like_count\", 0) + metrics.get(\"reply_count\", 0) + metrics.get(\"retweet_count\", 0) + metrics.get(\"quote_count\", 0)\n",
        "\n",
        "        # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ù„ÙŠ Ø¸Ù‡Ø±Øª Ø¨Ø§Ù„ØªÙˆÙŠØªØ©\n",
        "        matched_keywords = [kw for kw in all_keywords if kw.lower() in tweet.text.lower()]\n",
        "\n",
        "        data.append([\n",
        "            \"Twitter\",        # Ø§Ù„Ù…Ù†ØµØ©\n",
        "            tweet_url,        # URL\n",
        "            engagement,       # Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„ÙƒÙ„ÙŠ\n",
        "            tweet.text,       # Ø§Ù„Ù†Øµ\n",
        "            \", \".join(matched_keywords) if matched_keywords else \"N/A\"\n",
        "        ])\n",
        "\n",
        "# -------- 5) Save to CSV --------\n",
        "df = pd.DataFrame(data, columns=[\"Platform\", \"URL\", \"Engagement\", \"Text\", \"Matched Keywords\"])\n",
        "df.to_csv(\"twitter_data.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"save in the file\", len(df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsDdkpkl24P4",
        "outputId": "cc05d8b0-26d1-495d-9e0a-f8a953bbbce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save in the file 99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù ÙˆØ¹Ø±Ø¶ Ø§ÙˆÙ„ Ø®Ù…Ø³ Ø§Ø³Ø·Ø±"
      ],
      "metadata": {
        "id": "nLpi6tGKBIIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù ØªÙˆÙŠØªØ±)\n",
        "df = pd.read_csv(\"twitter_data.csv\")\n",
        "\n",
        "# Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 ØµÙÙˆÙ\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5zMYgEr-p_4",
        "outputId": "7d70a197-c08a-4d23-99a2-ab8608f6373a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Platform                                                URL  Engagement  \\\n",
            "0  Twitter  https://twitter.com/user/status/19701368662560...         130   \n",
            "1  Twitter  https://twitter.com/user/status/19701368661218...        1088   \n",
            "2  Twitter  https://twitter.com/user/status/19701368656481...        1219   \n",
            "3  Twitter  https://twitter.com/user/status/19701368656229...        1151   \n",
            "4  Twitter  https://twitter.com/user/status/19701368655431...           0   \n",
            "\n",
            "                                                Text Matched Keywords  \n",
            "0  RT @MikeCrispi: LIVE on the ground outside the...              NaN  \n",
            "1  RT @AdamMoczar: âš¡ğŸ‡ºğŸ‡² Charlie Kirk: â€œIslam is NO...          Muslims  \n",
            "2  RT @aldamu_jo: Israeli police clashed with ult...             Jews  \n",
            "3  RT @AzatAlsalim: A Hijabi woman tells a 'Gay f...              NaN  \n",
            "4  @ScotGovFM @JohnSwinney @PalMissionUK Absolute...         shameful  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø¨Ø¯ÙŠÙ†Ø§ Ø¹Ù…Ù„ÙŠØ© ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ùˆ Ø´ÙŠØ¡ Ø·Ø¨Ù‚Ù†Ø§Ù‡\n",
        "1.   Ø§Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©\n",
        "2.   Ø§Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ©\n",
        "3. Ø§Ø¹Ø§Ø¯Ø© Ø§Ù„ÙÙ‡Ø±Ø³Ø©"
      ],
      "metadata": {
        "id": "hMir3sJOBMJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙƒØ±Ø±Ø©\n",
        "df = df.drop_duplicates(subset=\"Text\")\n",
        "\n",
        "# Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ©\n",
        "df = df.dropna(subset=[\"Text\"])\n",
        "\n",
        "# Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ÙÙ‡Ø±Ø³Ø©\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "print(\"after cleaning\", df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UwsoqNu_E5i",
        "outputId": "07d417bd-87e2-47be-ebac-dfd405cb8a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after cleaning (90, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø§Ø¶ÙÙ†Ø§ Ù„Ø§Ø¨Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© Detoxify\n",
        "ÙˆØ·Ø¨Ø¹Ù†Ø§ Ø§ÙˆÙ„ Ù¡Ù  Ø§Ø¹Ù…Ø¯Ø©"
      ],
      "metadata": {
        "id": "nN2h_j5bBl1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install detoxify\n",
        "\n",
        "from detoxify import Detoxify\n",
        "\n",
        "# Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø®Ø·Ø§Ø¨ Ø§Ù„ÙƒØ±Ø§Ù‡ÙŠØ©\n",
        "model = Detoxify('original')\n",
        "\n",
        "# Ù†Ø­Ø³Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø£Ù† Ø§Ù„Ù†Øµ ÙÙŠÙ‡ \"toxic/hate\"\n",
        "df[\"toxicity_score\"] = df[\"Text\"].apply(lambda x: model.predict(str(x))[\"toxicity\"])\n",
        "\n",
        "# Ù†Ø­ÙˆÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Label (1 = Hate Speech, 0 = Not Hate Speech)\n",
        "df[\"Label\"] = df[\"toxicity_score\"].apply(lambda x: 1 if x > 0.5 else 0)\n",
        "\n",
        "print(df[[\"Text\", \"toxicity_score\", \"Label\"]].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3-JJVtl_PO_",
        "outputId": "d7235b94-35bb-4b65-8787-2d2d98dc7c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: detoxify in /usr/local/lib/python3.12/dist-packages (0.5.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from detoxify) (4.56.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from detoxify) (2.8.0+cu126)\n",
            "Requirement already satisfied: sentencepiece>=0.1.94 in /usr/local/lib/python3.12/dist-packages (from detoxify) (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->detoxify) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->detoxify) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers->detoxify) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers->detoxify) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->detoxify) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->detoxify) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->detoxify) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->detoxify) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->detoxify) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->detoxify) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->detoxify) (1.1.10)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.7.0->detoxify) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.7.0->detoxify) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->detoxify) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->detoxify) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->detoxify) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->detoxify) (2025.8.3)\n",
            "                                                Text  toxicity_score  Label\n",
            "0  RT @MikeCrispi: LIVE on the ground outside the...        0.020658      0\n",
            "1  RT @AdamMoczar: âš¡ğŸ‡ºğŸ‡² Charlie Kirk: â€œIslam is NO...        0.125268      0\n",
            "2  RT @aldamu_jo: Israeli police clashed with ult...        0.013052      0\n",
            "3  RT @AzatAlsalim: A Hijabi woman tells a 'Gay f...        0.426302      0\n",
            "4  @ScotGovFM @JohnSwinney @PalMissionUK Absolute...        0.018147      0\n",
            "5  RT @arthurwatkins: A lot of those that are say...        0.040267      0\n",
            "6  Just listen to these vile evil men! https://t....        0.885213      1\n",
            "7                @JoJoFromJerz Define \"hate speech.\"        0.020169      0\n",
            "8  RT @joshxhowie: Throwing chicken bones at a 77...        0.384480      0\n",
            "9  Do you know why itâ€™s called a daemon thread? A...        0.000682      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø­Ø°ÙÙ†Ø§ Ø§Ù„ url Ù…Ù† Ø§Ù„ÙƒÙ„Ø§Ù… Ø¹Ø´Ø§Ù† Ù†Ø­ÙˆÙ„Ù‡ Ø§Ù„Ù‰ ÙƒÙ„ÙŠÙ† ØªÙƒØ³Øª"
      ],
      "metadata": {
        "id": "qZt5OEXNBxGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_urls(text):\n",
        "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "df['clean_text'] = df['Text'].apply(remove_urls)"
      ],
      "metadata": {
        "id": "bmuGlUJ6_9d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø´Ù„Ù†Ø§ Ø§Ù„ÙŠÙˆØ²Ø± Ù†ÙŠÙ… Ø¹Ø´Ø§Ù† Ù†Ø³ÙˆÙŠ ÙƒÙ„ÙŠÙ† ØªÙƒØ³Øª"
      ],
      "metadata": {
        "id": "77oBpkLFB4Yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_mentions(text):\n",
        "    return re.sub(r'@\\w+', '', text)\n",
        "\n",
        "df['clean_text'] = df['clean_text'].apply(remove_mentions)\n"
      ],
      "metadata": {
        "id": "EUat0xHFAExG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø´Ù„Ù†Ø§ Ø§Ù„Ø§ÙŠÙ…ÙˆØ¬ÙŠ ÙˆØ§ÙŠ Ø±Ù…ÙˆØ² Ù„Ø§Ù† Ù…Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§"
      ],
      "metadata": {
        "id": "sBbXH651CAfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_emojis(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # ÙˆØ¬ÙˆÙ‡ Ø¶Ø§Ø­ÙƒØ©\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # Ø±Ù…ÙˆØ² ÙˆØ£ÙŠÙ‚ÙˆÙ†Ø§Øª\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # Ù…Ø±ÙƒØ¨Ø§Øª\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # Ø£Ø¹Ù„Ø§Ù…\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "df['clean_text'] = df['clean_text'].apply(remove_emojis)\n"
      ],
      "metadata": {
        "id": "Tng2zd9HAP94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø­ÙˆÙ„Ù†Ø§ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‰ Lowercase"
      ],
      "metadata": {
        "id": "S1wpBvTZCE2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_text'] = df['clean_text'].str.lower()\n"
      ],
      "metadata": {
        "id": "CGTKu45DASgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø´Ù„Ù†Ø§ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ…"
      ],
      "metadata": {
        "id": "8uEk1BKlCKpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "df['clean_text'] = df['clean_text'].apply(remove_punctuation)\n"
      ],
      "metadata": {
        "id": "TGe4bXgDAUlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù€ Stopwords"
      ],
      "metadata": {
        "id": "voXL-yx-COpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "df['clean_text'] = df['clean_text'].apply(remove_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4sxSEsuChF_",
        "outputId": "6b324084-66b3-4d2e-dbf8-95c5bb8599d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø­Ø³Ø¨Ù†Ø§ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª\n",
        "\"Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ \"\n",
        "\n"
      ],
      "metadata": {
        "id": "mJt6ZH4bCktr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['word_count'] = df['clean_text'].apply(lambda x: len(x.split()))\n"
      ],
      "metadata": {
        "id": "z5Qe8SE9AY5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø­Ø³Ø¨Ù†Ø§ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ù„Ø¨ÙŠØ©"
      ],
      "metadata": {
        "id": "dl1hQn7tCS5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words = ['disgusting', 'horrible', 'ugly', 'stupid', 'hate']\n",
        "\n",
        "def count_negative_words(text):\n",
        "    return sum(word in negative_words for word in text.split())\n",
        "\n",
        "df['negative_word_count'] = df['clean_text'].apply(count_negative_words)\n"
      ],
      "metadata": {
        "id": "X1aHYQF_AZtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ù†Ø´ÙˆÙ Ù„Ùˆ ÙÙŠÙ‡ ØªÙˆØ§Ø²Ù† Ù…Ø¹ Ø§Ù„Ù„ÙŠØ¨Ù„"
      ],
      "metadata": {
        "id": "Bf7yfs0xCUoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdn3aVgWAcSm",
        "outputId": "fbdd590d-7b7c-4ad3-c387-85d7d8d0430e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "0    65\n",
            "1    25\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø¹Ù…Ù„ Sampling Ø¥Ø°Ø§ ÙÙŠÙ‡ Ø§Ù†Ø­ÙŠØ§Ø²"
      ],
      "metadata": {
        "id": "RTVSl2reCuzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Ù†ÙØµÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù„ÙŠØ¨Ù„\n",
        "df_majority = df[df['Label'] == 0]\n",
        "df_minority = df[df['Label'] == 1]\n",
        "\n",
        "# Ù†Ø¹Ù…Ù„ oversampling Ù„Ù„Ù€ minority\n",
        "df_minority_upsampled = resample(df_minority,\n",
        "                                 replace=True,     # Ø³Ø­Ø¨ Ù…Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±\n",
        "                                 n_samples=len(df_majority),\n",
        "                                 random_state=42)\n",
        "\n",
        "# Ù†Ø¯Ù…Ø¬ Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠÙ†\n",
        "df_balanced = pd.concat([df_majority, df_minority_upsampled])"
      ],
      "metadata": {
        "id": "aV8HCBjLAjXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø­ÙØ¸Ù†Ø§ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ†Ø¸ÙŠÙ"
      ],
      "metadata": {
        "id": "bcebhVfMCynk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©\n",
        "df.to_csv(\"twitter_labeled.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Save\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02D8OiZ9AwBy",
        "outputId": "08ba62d4-1279-4aea-e33b-1d3f6a8d9179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}